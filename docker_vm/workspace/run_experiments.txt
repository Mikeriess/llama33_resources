VLM Finetuning Experiments Runner

This script runs multiple finetuning experiments based on a JSON configuration file, with continuous model improvement.

Configuration File Format (experiments.json):
{
    "dataset1": [
        "instruction1",
        "instruction2"
    ],
    "dataset2": [
        "instruction1",
        "instruction2",
        "instruction3"
    ]
}

Basic Usage:
python run_experiments.py --hf_token YOUR_HUGGINGFACE_TOKEN

Optional Arguments:
--config: Path to experiments configuration file (default: experiments.json)
--runs_per_config: Number of runs for each dataset-instruction combination (default: 1)
--hf_token: HuggingFace token for model upload (required)

Examples:
1. Run with default configuration:
python run_experiments.py --hf_token hf_xxx

2. Run with custom configuration file:
python run_experiments.py --config my_experiments.json --hf_token hf_xxx

Notes:
- Each experiment continues from the previous experiment's checkpoint
- Models are saved locally as "llama32_checkpoint"
- Models are uploaded to HuggingFace Hub as "llama32_<experiment_number>_<dataset_name>"
- Each experiment will be logged separately to Weights & Biases
- Each run gets a unique timestamp
- The script will continue with remaining experiments if one fails
- Progress is displayed in the console
- Configuration file must be valid JSON with datasets as keys and instruction lists as values

Requirements:
Same as multigpu_finetune_vlm.py plus:
- huggingface_hub
- Valid HuggingFace token with write access 